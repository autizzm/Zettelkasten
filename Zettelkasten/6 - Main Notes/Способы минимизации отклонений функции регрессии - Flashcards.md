
Theory for the cards: [[Способы минимизации отклонений функции регрессии]]

FILE TAGS: math_stat

Q: Какие есть способы минимизации отклонений функции?
A: Из того, что я должен знать:
- МНК - метод наименьших квадратов:
	
	$\sum_{i=1}^n e_i^2 = \sum_{i=1}^n (y_i - \hat{y_i})^2 = \sum_{i=1}^n (y_i - b_0 - b_1x_i)^2$
	
- МНМ - метод наименьших модулей:
	
	$\sum_{i=1}^n|e_i| = \sum_{i=1}^n |y_i - \hat{y_i}| = \sum_{i=1}^n |y_i - b_0 - b_1x_i|$
	
Из того, что знать не обязательно:
- Метод Моментов (ММ)
- Метод Максимального Правдоподобия (ММП)
<!--ID: 1759653234104-->


Q: Почему для минимизации отклонений функции мы не можем минимизировать просто сумму отклонений?
$\sum_{i=1}^n e_i = \sum_{i=1}^n (y_i - \hat{y_i}) = \sum_{i=1}^n(y_i - b_0 - b_1x_i)$
A: Потому, что отклонения имеют знак, и, если мы будем суммировать отклонения и искать их минимум, то противоположные отклонения компенсирубт друг друга и график такой функции вместо того, чтобы проходить максимально близко от всех точек будет проходить равноудалённо от противоположных. 
	
Если размышлять чисто алгебраически, то это аннулирует противоположные отклонения вообще и мы можем получить суммарное отклонение равное хоть нулю, хотя значения СВ будут вовсе не на графике такой функции.
<!--ID: 1759653234113-->
