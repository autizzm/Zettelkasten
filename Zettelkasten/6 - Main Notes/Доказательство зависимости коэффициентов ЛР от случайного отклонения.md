
16-10-2025 09:12

Status: #child 

Tags: [[ТВиМС]]

---
# Доказательство зависимости коэффициентов ЛР от случайного отклонения

### Предположения

Надежность получаемых оценок, очевидно, тесно связана с дисперсией случайных отклонений $ε_i$.

Фактически $D(ε_i)$ является дисперсией $D(Y|X = x_i)$ переменной $Y$ относительно линии регрессии (дисперсией $Y$, очищенной от влияния $X$)

~={pink}Почему мы считаем, что коэффициенты не зависят от X, а только от Y, которая зависит от случайного отклонения, т.е.Y, очищенная от влияния X?=~
	т.к. $X$ не считается случайной при определении коэффициентов - Это [[Классические предпосылки МНК#Дополнительныые предположения МНК|допущение МНК]]


> **Дисперсия $Y$, очищенная от влияния $X$** — это часть вариации (изменчивости) случайной величины $Y$, **не объясняемая** переменной $X$.
> 
> Если между $Y$ и $X$ предполагается **линейная зависимость**, то:
>
>$Y = a + bX + \varepsilon$
>
>где
> - $a + bX$ — предсказанное (объяснённое) значение,
> - $\varepsilon$ — случайная ошибка (остаток), не связанная с $X$.
>
>**Общая дисперсия** $Y$:
>
>$\mathrm{Var}(Y)$
>
>**Объяснённая дисперсия** (часть вариации $Y$, связанная с $X$):
>
>$\mathrm{Var}(\hat{Y})$
>
>**Очищенная (остаточная) дисперсия**:
>
>$\mathrm{Var}(\varepsilon) = \mathrm{Var}(Y) - \mathrm{Var}(\hat{Y})$

По [[Классические предпосылки МНК|третьей предпосылке МНК]] - полагаем, что все эти дисперсии равны:

### $D(\varepsilon_i) = y_e^2 = \sigma^2$

где
- $\sigma^2$ - истинная оценка ошибок $\varepsilon$
- $y^2_e$ - оценка этой дисперсии по выборке (дисперсия $Y$, очищенная от влияния $X$).


![[Pasted image 20251016094254.png]]
![[Pasted image 20251016094337.png]]

~={pink}Почему считаем, считаем, что $\sum D(y_i) = \sigma^2$, где $\sigma^2$ - [[Оценка дисперсии случайных отклонений (Стандартная ошибка регрессии)|дисперсия отклонений ЛР]]?=~
	Потому что:
	- $y_i = \text{константа} + \varepsilon_i$​
	- $\operatorname{Var}(\text{константа} + \varepsilon_i) = \operatorname{Var}(\varepsilon_i) = \sigma^2.$




## Выводы:


- **Чем больше число n наблюдений, тем меньше дисперсии оценок. Это вполне логично, т. к. чем большим числом мы располагаем, тем вероятнее получение более точных оценок.**

- **Чем больше дисперсия (разброс значений $\sum (х_i − \bar{x})^2$) объясняющей переменной, тем меньше дисперсия оценок коэффициентов. Другими словами, чем шире область изменений объясняющей переменной, тем точнее будут оценки (тем меньше доля случайности в их определении).**

![[Pasted image 20251016144801.png]]

Например, на рисунке через пары точек (1, 3) и (2, 3) проведена одна и та же прямая. Но диапазон (1, 3) шире диапазона (2, 3). 
Если вместо точки 3 рассмотреть либо точку 3а, либо 3б (т. е. при случайном изменении выборки), то наклон прямой для пары (1, 3) изменится значительно меньше, чем для        пары (2, 3).


- **Дисперсии $b_0$ и $b_1$ прямо пропорциональны дисперсии случайного отклонения $\sigma_2$. Следовательно, чем больше фактор случайности, тем менее точными будут оценки.**

![[Pasted image 20251016135641.png]]
 Если все точки лежат на одной прямой,  т.е. $e_i = 0$, то прямая определяется однозначно и ошибки определения не будет вовсе: $\sum e_i = 0$  =>  $S^2 = 0$  =>  $S_{b_0} = S_{b_1} = 0$

![[Pasted image 20251016140006.png]]
Если все точки не лежат на одной прямой, то для трёх точек прямая будет той же, но для двух точек ((1, 2), (1, 3), (2, 3)) прямые регрессии будут существенно отличаться. Следовательно, значительно различаются их углы наклона, а значит, стандартная ошибка $S_{b1}$ коэффициента регрессии $b_1$ будет существенной.

- **Разброс значений свободного члена $b_0$ тем больще, чем больше средняя величина $\bar{ x^2 }$.**

Это связано с тем, что при больших по модулю значениях $X$ даже небольшое изменение наклона регрессионной прямой может вызвать большое изменение оценки свободного члена, поскольку в этом случае в среднем велико расстояние от точек наблюдений до оси $OY$.
![[Pasted image 20251016145526.png]]

На рисунке через пары точек (1, 2) и (3, 4) проходит одна и та же прямая, пересекающая ось OY в точке (0, $b_0$). Для второй из этих пар значения переменной X больше по абсолютной величине (при одинаковом диапазоне изменений X и Y), чем для первой. Если в этих парах точки 1 и 3 изменить на одну и ту же величину (новые точки 1а, 3а), то углы наклона новых прямых (1а, 2) и (3а, 4) будут одинаковы. Но свободный член $b_{01}$ для первой прямой будет существенно меньше отличаться от $b_0$, чем свободный член $b_{_02}$ для второй прямой.


---
### **TL; DR:**

- Дисперсия коэффициентов зависит от $Y$, что мы вывели из их формул по МНК.
- отсюда получаем выводы. Их следует [[Доказательство зависимости коэффициентов ЛР от случайного отклонения#Выводы|почитать]].


----
#### [[Доказательство зависимости коэффициентов ЛР от случайного отклонения - Flashcards|Link to flashcards]]



---
### References:

- [[Оценка дисперсии случайных отклонений (Стандартная ошибка регрессии)]]