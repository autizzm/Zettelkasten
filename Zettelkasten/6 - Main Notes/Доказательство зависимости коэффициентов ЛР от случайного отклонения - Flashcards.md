
Theory for the cards: [[Доказательство зависимости коэффициентов ЛР от случайного отклонения]]

FILE TAGS: math_stat

Q: От чего зависит качество оценки коэффициентов ЛР (по МНК)?
A: Т.к. при оценивании коэффициентов мы делаем допущение, что $X$ - не случайная величина, то оценки (которые в этом случае являются случайными величинами) зависят от случайных отклонений.
	
Это доказывается через формулы МНК, посредством преобразований.
	
Зависимость самих коэффициентов от $Y$:
	
$b_1 = \sum_{i =1}^n \frac{(x_i - \bar{x})}{ (x_i - \bar{x})^2}y_i$
	
 $b_0 = \sum (\frac{1}{n} - \frac{(x_i - \bar{x})\bar{x}}{(x_i - \bar{x})^2})y_i$
	
После чего находим дисперсию этих коэффициентов (мы можем это сделать, т.к. коэффицииенты являются СВ, т.к. зависят от отклонений).  
	
Заметь, здесь $\sum D(y_i) = \sum D(\varepsilon_i) = \sigma^2$ потому что мы считаем $X$ константой (не СВ), а по свойствам дисперсии:
	- $y_i = \text{константа} + \varepsilon_i$​
	- $\operatorname{Var}(\text{константа} + \varepsilon_i) = \operatorname{Var}(\varepsilon_i) = \sigma^2.$
	
$D(b_1) = \sigma^2 \sum (\frac{(x_i - \bar{x})\bar{x}}{(x_i - \bar{x})^2})^2 = \sigma^2 \frac{\sum (x_i - \bar{x})^2}{(\sum (x_i - \bar{x})^2)^2} = \frac{\sigma^2}{\sum (x_i - \bar{x})^2}$  
	
$D(b_0) = \sigma^2 \sum (\frac{1}{n} - \frac{(x_i - \bar{x})\bar{x}}{(x_i - \bar{x})^2})^2 = \frac{\sigma^2 \sum x_i^2}{n \sum (x_i - \bar{x})^2}$ 
<!--ID: 1760616795771-->



Q: Перечисли свойства оценок коэффициентов ЛР? (Выводы, которые мы получаем при анализе дисперсий коэффициентов)
A:  Вот сами формулы, шоб не втыкал:
	
$D(b_1) = \sigma^2 \sum (\frac{(x_i - \bar{x})\bar{x}}{(x_i - \bar{x})^2})^2 = \sigma^2 \frac{\sum (x_i - \bar{x})^2}{(\sum (x_i - \bar{x})^2)^2} = \frac{\sigma^2}{\sum (x_i - \bar{x})^2}$  
	
$D(b_0) = \sigma^2 \sum (\frac{1}{n} - \frac{(x_i - \bar{x})\bar{x}}{(x_i - \bar{x})^2})^2 = \frac{\sigma^2 \sum x_i^2}{n \sum (x_i - \bar{x})^2}$ 
	
- **Чем больше число n наблюдений, тем меньше дисперсии оценок. Это вполне логично, т. к. чем большим числом мы располагаем, тем вероятнее получение более точных оценок.**
	
	
- **Чем больше дисперсия (разброс значений $\sum (х_i − \bar{x})^2$) объясняющей переменной, тем меньше дисперсия оценок коэффициентов. Другими словами, чем шире область изменений объясняющей переменной, тем точнее будут оценки (тем меньше доля случайности в их определении).**
	
![[Pasted image 20251016144801.png]]
	
Например, на рисунке через пары точек (1, 3) и (2, 3) проведена одна и та же прямая. Но диапазон (1, 3) шире диапазона (2, 3). 
Если вместо точки 3 рассмотреть либо точку 3а, либо 3б (т. е. при случайном изменении выборки), то наклон прямой для пары (1, 3) изменится значительно меньше, чем для        пары (2, 3).
	
	
- **Дисперсии $b_0$ и $b_1$ прямо пропорциональны дисперсии случайного отклонения $\sigma_2$. Следовательно, чем больше фактор случайности, тем менее точными будут оценки. **
	
![[Pasted image 20251016135641.png]]
 Если все точки лежат на одной прямой,  т.е. $e_i = 0$, то прямая определяется однозначно и ошибки определения не будет вовсе: $\sum e_i = 0$  =>  $S^2 = 0$  =>  $S_{b_0} = S_{b_1} = 0$
	
![[Pasted image 20251016140006.png]]
Если все точки не лежат на одной прямой, то для трёх точек прямая будет той же, но для двух точек ((1, 2), (1, 3), (2, 3)) прямые регрессии будут существенно отличаться. Следовательно, значительно различаются их углы наклона, а значит, стандартная ошибка $S_{b1}$ коэффициента регрессии $b_1$ будет существенной.
	
	
- **Разброс значений свободного члена $b_0$ тем больше, чем больше средняя величина $\bar{ x^2 }$.**
	
Это связано с тем, что при больших по модулю значениях $X$ даже небольшое изменение наклона регрессионной прямой может вызвать большое изменение оценки свободного члена, поскольку в этом случае в среднем велико расстояние от точек наблюдений до оси $OY$.
	
![[Pasted image 20251016145526.png]]
	
На рисунке через пары точек (1, 2) и (3, 4) проходит одна и та же прямая, пересекающая ось OY в точке (0, $b_0$). Для второй из этих пар значения переменной X больше по абсолютной величине (при одинаковом диапазоне изменений X и Y), чем для первой. Если в этих парах точки 1 и 3 изменить на одну и ту же величину (новые точки 1а, 3а), то углы наклона новых прямых (1а, 2) и (3а, 4) будут одинаковы. Но свободный член $b_{01}$ для первой прямой будет существенно меньше отличаться от $b_0$, чем свободный член $b_{_02}$ для второй прямой.
<!--ID: 1760616795783-->

Q: Как зависит свободный коэффициент ЛР ($b_0$) от среднего выборочного $\bar{x}$?
A:  **Разброс значений свободного члена $b_0$ тем больше, чем больше средняя величина $\bar{ x^2 }$.**
	
Это связано с тем, что при больших по модулю значениях $X$ даже небольшое изменение наклона регрессионной прямой может вызвать большое изменение оценки свободного члена, поскольку в этом случае в среднем велико расстояние от точек наблюдений до оси $OY$.
	
![[Pasted image 20251016145526.png]]
	
На рисунке через пары точек (1, 2) и (3, 4) проходит одна и та же прямая, пересекающая ось OY в точке (0, $b_0$). Для второй из этих пар значения переменной X больше по абсолютной величине (при одинаковом диапазоне изменений X и Y), чем для первой. Если в этих парах точки 1 и 3 изменить на одну и ту же величину (новые точки 1а, 3а), то углы наклона новых прямых (1а, 2) и (3а, 4) будут одинаковы. Но свободный член $b_{01}$ для первой прямой будет существенно меньше отличаться от $b_0$, чем свободный член $b_{_02}$ для второй прямой.
<!--ID: 1760617397988-->



Q: Как коэффициенты ЛР зависят от случайного отклонения? Что хначит данная зависимость?
A:  **Дисперсии $b_0$ и $b_1$ прямо пропорциональны дисперсии случайного отклонения $\sigma_2$. Следовательно, чем больше фактор случайности, тем менее точными будут оценки.**
	
![[Pasted image 20251016135641.png]]
 Если все точки лежат на одной прямой,  т.е. $e_i = 0$, то прямая определяется однозначно и ошибки определения не будет вовсе: $\sum e_i = 0$  =>  $S^2 = 0$  =>  $S_{b_0} = S_{b_1} = 0$
	
![[Pasted image 20251016140006.png]]
Если все точки не лежат на одной прямой, то для трёх точек прямая будет той же, но для двух точек ((1, 2), (1, 3), (2, 3)) прямые регрессии будут существенно отличаться. Следовательно, значительно различаются их углы наклона, а значит, стандартная ошибка $S_{b1}$ коэффициента регрессии $b_1$ будет существенной.
<!--ID: 1760617398000-->


Q: Вляияет ли $X$ на оценку коэффициентов ЛР (на точность оценки)? 
A: При оценке коэффициентов мы делаем допущение, что $X$ - не случайная величина, то оценки (которые в этом случае являются случайными величинами) зависят от случайных отклонений. 
	
Однако, пусть $X$ и не считается случайной величиной, он влияет дисперсию оценок коэффициентов:
	
**Чем больше дисперсия (разброс значений $\sum (х_i − \bar{x})^2$) объясняющей переменной, тем меньше дисперсия оценок коэффициентов. Другими словами, чем шире область изменений объясняющей переменной, тем точнее будут оценки (тем меньше доля случайности в их определении).**
	
![[Pasted image 20251016144801.png]]
	
Например, на рисунке через пары точек (1, 3) и (2, 3) проведена одна и та же прямая. Но диапазон (1, 3) шире диапазона (2, 3). 
Если вместо точки 3 рассмотреть либо точку 3а, либо 3б (т. е. при случайном изменении выборки), то наклон прямой для пары (1, 3) изменится значительно меньше, чем для        пары (2, 3).
<!--ID: 1760617398009-->
